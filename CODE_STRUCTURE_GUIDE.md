# æ©Ÿå™¨å­¸ç¿’ç¨‹å¼ç¢¼çµæ§‹èªªæ˜æŒ‡å—
# Machine Learning Code Structure Guide

## ğŸ“‹ ç¨‹å¼æª”æ¡ˆèªªæ˜

### ä¸»è¦ç¨‹å¼æª”æ¡ˆ
- **`mike_SVM_withoutBin.py`**: åŸå§‹ç‰ˆæœ¬ï¼ˆåŠŸèƒ½å®Œæ•´ä½†è¨»è§£è¼ƒå°‘ï¼‰
- **`mike_SVM_withoutBin_annotated.py`**: è©³ç´°è¨»è§£ç‰ˆæœ¬ï¼ˆå»ºè­°å­¸ç¿’ä½¿ç”¨ï¼‰

## ğŸ—ï¸ ç¨‹å¼æ¶æ§‹èªªæ˜

### æ•´é«”æµç¨‹ (Overall Flow)
```
è³‡æ–™è¼‰å…¥ â†’ ç‰¹å¾µå·¥ç¨‹ â†’ è³‡æ–™åˆ†å‰² â†’ é¡åˆ¥å¹³è¡¡ â†’ 
æ¨¡å‹è¨“ç·´ â†’ æ€§èƒ½è©•ä¼° â†’ çµæœæ¯”è¼ƒ â†’ è¦–è¦ºåŒ– â†’ åˆ†æå ±å‘Š
```

### è©³ç´°æ­¥é©Ÿè§£æ

#### ç¬¬1æ­¥ï¼šå¥—ä»¶å°å…¥ (Package Imports)
```python
# æ•¸æ“šè™•ç†
import pandas as pd
import numpy as np

# è¦–è¦ºåŒ–
import matplotlib.pyplot as plt
import seaborn as sns

# æ©Ÿå™¨å­¸ç¿’
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
```

**é‡é»æ¦‚å¿µ:**
- `pandas`: è³‡æ–™æ¡†æ“ä½œçš„æ¨™æº–å·¥å…·
- `sklearn`: Pythonæ©Ÿå™¨å­¸ç¿’çš„æ ¸å¿ƒå¥—ä»¶
- `imblearn`: è™•ç†ä¸å¹³è¡¡è³‡æ–™é›†çš„å°ˆç”¨å¥—ä»¶

#### ç¬¬2æ­¥ï¼šè³‡æ–™è¼‰å…¥ (Data Loading)
```python
df_drug = pd.read_csv('drug200.csv')
df_drug_original = df_drug.copy()  # å‚™ä»½åŸå§‹è³‡æ–™
```

**é‡é»æ¦‚å¿µ:**
- ç¸½æ˜¯å‚™ä»½åŸå§‹è³‡æ–™
- æª¢æŸ¥è³‡æ–™å¤§å°ã€æ¬„ä½ã€é¡åˆ¥åˆ†å¸ƒ

#### ç¬¬3æ­¥ï¼šç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
```python
X = df_drug.drop(["Drug"], axis=1)  # ç‰¹å¾µè®Šæ•¸
y = df_drug["Drug"]                 # ç›®æ¨™è®Šæ•¸
X = pd.get_dummies(X)               # One-hotç·¨ç¢¼
```

**é‡é»æ¦‚å¿µ:**
- **One-hotç·¨ç¢¼**: å°‡é¡åˆ¥è®Šæ•¸è½‰ç‚ºæ•¸å€¼è®Šæ•¸
  - ä¾‹å¦‚: 'Sex' â†’ 'Sex_F', 'Sex_M'
  - æ©Ÿå™¨å­¸ç¿’ç®—æ³•åªèƒ½è™•ç†æ•¸å€¼è³‡æ–™

#### ç¬¬4æ­¥ï¼šè³‡æ–™åˆ†å‰² (Data Splitting)
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0
)
```

**é‡é»æ¦‚å¿µ:**
- **è¨“ç·´é›†**: ç”¨æ–¼è¨“ç·´æ¨¡å‹ (70%)
- **æ¸¬è©¦é›†**: ç”¨æ–¼è©•ä¼°æ¨¡å‹æ€§èƒ½ (30%)
- **random_state**: ç¢ºä¿çµæœå¯é‡ç¾

#### ç¬¬5æ­¥ï¼šè™•ç†é¡åˆ¥ä¸å¹³è¡¡ (Handle Class Imbalance)
```python
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)
```

**é‡é»æ¦‚å¿µ:**
- **SMOTE**: åˆæˆå°‘æ•¸é¡åˆ¥éæ¡æ¨£æŠ€è¡“
- å¹³è¡¡å„é¡åˆ¥æ¨£æœ¬æ•¸ï¼Œæé«˜æ¨¡å‹å…¬å¹³æ€§
- åªåœ¨è¨“ç·´é›†ä½¿ç”¨ï¼Œæ¸¬è©¦é›†ä¿æŒåŸå§‹åˆ†å¸ƒ

#### ç¬¬6æ­¥ï¼šæ¨¡å‹å®šç¾© (Model Definition)
```python
classifiers = {
    'Linear SVM': SVC(kernel='linear', random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    # ... æ›´å¤šæ¨¡å‹
}
```

**æ¨¡å‹é¸æ“‡èªªæ˜:**
- **Linear SVM**: ç·šæ€§åˆ†é¡ï¼Œè§£é‡‹æ€§å¥½
- **RBF SVM**: éç·šæ€§åˆ†é¡ï¼Œè™•ç†è¤‡é›œé‚Šç•Œ
- **Random Forest**: é›†æˆæ–¹æ³•ï¼Œç©©å®šæ€§ä½³
- **Gradient Boosting**: åºåˆ—é›†æˆï¼Œæº–ç¢ºç‡é«˜
- **Neural Network**: æ·±åº¦å­¸ç¿’ï¼Œé©åˆè¤‡é›œæ¨¡å¼

#### ç¬¬7æ­¥ï¼šæ¨¡å‹è¨“ç·´å¾ªç’° (Model Training Loop)
```python
for name, classifier in classifiers.items():
    # è¨˜éŒ„è¨“ç·´æ™‚é–“
    start_time = time.time()
    classifier.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # é€²è¡Œé æ¸¬
    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # å„²å­˜çµæœ
    results[name] = {
        'accuracy': accuracy,
        'train_time': train_time,
        'predictions': y_pred
    }
```

**é‡é»æ¦‚å¿µ:**
- çµ±ä¸€çš„è¨“ç·´æµç¨‹ç¢ºä¿å…¬å¹³æ¯”è¼ƒ
- è¨˜éŒ„å¤šç¨®æ€§èƒ½æŒ‡æ¨™
- çµæ§‹åŒ–å„²å­˜çµæœä¾¿æ–¼å¾ŒçºŒåˆ†æ

#### ç¬¬8æ­¥ï¼šè¦–è¦ºåŒ– (Visualization)
```python
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# å­åœ–1: æº–ç¢ºç‡æ¯”è¼ƒ
axes[0,0].bar(methods, accuracies)

# å­åœ–2: è¨“ç·´æ™‚é–“æ¯”è¼ƒ
axes[0,1].bar(methods, train_times)

# å­åœ–3: æº–ç¢ºç‡ vs é€Ÿåº¦æ•£é»åœ–
axes[1,0].scatter(train_times, accuracies)

# å­åœ–4: æœ€ä½³æ–¹æ³•æ··æ·†çŸ©é™£
axes[1,1].imshow(confusion_matrix)
```

**åœ–è¡¨é¡å‹èªªæ˜:**
- **é•·æ¢åœ–**: æ¯”è¼ƒä¸åŒæ–¹æ³•çš„å–®ä¸€æŒ‡æ¨™
- **æ•£é»åœ–**: å±•ç¤ºå…©å€‹æŒ‡æ¨™é–“çš„é—œä¿‚
- **æ··æ·†çŸ©é™£**: è©³ç´°åˆ†æåˆ†é¡éŒ¯èª¤æ¨¡å¼

## ğŸ¯ é—œéµæ©Ÿå™¨å­¸ç¿’æ¦‚å¿µ

### 1. è¨“ç·´ vs æ¸¬è©¦
- **è¨“ç·´é›†**: æ¨¡å‹å­¸ç¿’çš„è³‡æ–™
- **æ¸¬è©¦é›†**: è©•ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„è³‡æ–™
- **é‡è¦æ€§**: é¿å…éåº¦é…é©(overfitting)

### 2. äº¤å‰é©—è­‰ (Cross-Validation)
- æ›´å¯é çš„æ€§èƒ½è©•ä¼°æ–¹æ³•
- å°‡è³‡æ–™åˆ†æˆå¤šæŠ˜é€²è¡Œè¨“ç·´å’Œé©—è­‰
- æ¸›å°‘éš¨æ©Ÿæ€§å½±éŸ¿

### 3. ç‰¹å¾µç¸®æ”¾ (Feature Scaling)
- æŸäº›ç®—æ³•éœ€è¦ç‰¹å¾µåœ¨ç›¸ä¼¼å°ºåº¦ä¸Š
- æ¨™æº–åŒ–: (x - mean) / std
- æ­£è¦åŒ–: (x - min) / (max - min)

### 4. è¶…åƒæ•¸èª¿æ•´ (Hyperparameter Tuning)
- æ¨¡å‹æ€§èƒ½çš„é—œéµå› ç´ 
- æ–¹æ³•: ç¶²æ ¼æœç´¢ã€éš¨æ©Ÿæœç´¢ã€è²è‘‰æ–¯å„ªåŒ–
- éœ€è¦åœ¨é©—è­‰é›†ä¸Šé€²è¡Œ

## ğŸ“Š è©•ä¼°æŒ‡æ¨™èªªæ˜

### 1. æº–ç¢ºç‡ (Accuracy)
```
æº–ç¢ºç‡ = æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸
```
- æœ€ç›´è§€çš„æŒ‡æ¨™
- é¡åˆ¥å¹³è¡¡æ™‚è¼ƒç‚ºå¯é 

### 2. ç²¾ç¢ºç‡ (Precision)
```
ç²¾ç¢ºç‡ = çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡æ­£ä¾‹)
```
- é æ¸¬ç‚ºæ­£ä¾‹ä¸­å¯¦éš›ç‚ºæ­£ä¾‹çš„æ¯”ä¾‹

### 3. å¬å›ç‡ (Recall)
```
å¬å›ç‡ = çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡è² ä¾‹)
```
- å¯¦éš›æ­£ä¾‹ä¸­è¢«æ­£ç¢ºè­˜åˆ¥çš„æ¯”ä¾‹

### 4. F1åˆ†æ•¸ (F1-Score)
```
F1 = 2 * (ç²¾ç¢ºç‡ * å¬å›ç‡) / (ç²¾ç¢ºç‡ + å¬å›ç‡)
```
- ç²¾ç¢ºç‡å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡

## ğŸ› ï¸ ç¨‹å¼ç¢¼å„ªåŒ–å»ºè­°

### 1. æ¨¡çµ„åŒ–è¨­è¨ˆ
```python
def load_and_preprocess_data():
    # è³‡æ–™è¼‰å…¥å’Œé è™•ç†é‚è¼¯
    return X, y

def train_models(X_train, y_train):
    # æ¨¡å‹è¨“ç·´é‚è¼¯
    return results

def visualize_results(results):
    # è¦–è¦ºåŒ–é‚è¼¯
    pass
```

### 2. åƒæ•¸é…ç½®
```python
CONFIG = {
    'TEST_SIZE': 0.3,
    'RANDOM_STATE': 42,
    'SMOTE_RANDOM_STATE': 42,
    'MODEL_PARAMS': {
        'SVM': {'kernel': 'linear', 'max_iter': 251},
        'RF': {'n_estimators': 100}
    }
}
```

### 3. éŒ¯èª¤è™•ç†
```python
try:
    results = train_model(X_train, y_train)
except Exception as e:
    print(f"æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
    continue
```

## ğŸ” é™¤éŒ¯æŠ€å·§

### 1. è³‡æ–™æª¢æŸ¥
```python
print(f"è³‡æ–™å½¢ç‹€: {X.shape}")
print(f"ç¼ºå¤±å€¼: {X.isnull().sum()}")
print(f"é¡åˆ¥åˆ†å¸ƒ: {y.value_counts()}")
```

### 2. æ¨¡å‹è¨ºæ–·
```python
# æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸè¨“ç·´
if hasattr(model, 'feature_importances_'):
    print("ç‰¹å¾µé‡è¦æ€§:", model.feature_importances_)

# æª¢æŸ¥é æ¸¬åˆç†æ€§
print("é æ¸¬é¡åˆ¥:", np.unique(y_pred))
print("å¯¦éš›é¡åˆ¥:", np.unique(y_test))
```

### 3. æ€§èƒ½åˆ†æ
```python
# æª¢æŸ¥å„é¡åˆ¥æ€§èƒ½
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

## ğŸ“š é€²éšå­¸ç¿’æ–¹å‘

### 1. ç‰¹å¾µé¸æ“‡
- çµ±è¨ˆæ–¹æ³•: å¡æ–¹æª¢é©—ã€ANOVA
- åµŒå…¥æ–¹æ³•: Lassoã€Ridge
- åŒ…è£æ–¹æ³•: éæ­¸ç‰¹å¾µæ¶ˆé™¤

### 2. é›†æˆå­¸ç¿’
- Bagging: Random Forest, Extra Trees
- Boosting: AdaBoost, Gradient Boosting, XGBoost
- Stacking: å¤šå±¤æ¨¡å‹çµ„åˆ

### 3. æ·±åº¦å­¸ç¿’
- ç¥ç¶“ç¶²è·¯æ¶æ§‹è¨­è¨ˆ
- æ­£å‰‡åŒ–æŠ€è¡“
- å„ªåŒ–ç®—æ³•é¸æ“‡

### 4. æ¨¡å‹è§£é‡‹æ€§
- SHAPå€¼åˆ†æ
- LIMEå±€éƒ¨è§£é‡‹
- ç‰¹å¾µé‡è¦æ€§æ’åº